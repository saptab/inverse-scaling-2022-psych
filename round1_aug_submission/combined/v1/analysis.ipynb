{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_v1.csv\").drop(columns=\"Unnamed: 0\")\n",
    "df[\"n_classes\"] = df.classes.str.count(\",\") + 1\n",
    "df[\"baseline_acc\"] = 1 / df.n_classes\n",
    "df[\"baseline_loss\"] = -np.log(df.baseline_acc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.n_classes.hist(bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_MAP: dict[str, str] = {\n",
    "    \"ada\": \"combined_v1_gpt3/classification/ada.csv\",\n",
    "    \"babbage\": \"combined_v1_gpt3/classification/babbage.csv\",\n",
    "    \"curie\": \"combined_v1_gpt3/classification/curie.csv\",\n",
    "    \"davinci\": \"combined_v1_gpt3/classification/davinci.csv\",\n",
    "    \"ada_rlhf\": \"combined_v1_gpt3_rlhf/classification/text-ada-001.csv\",\n",
    "    \"babbage_rlhf\": \"combined_v1_gpt3_rlhf/classification/text-babbage-001.csv\",\n",
    "    \"curie_rlhf\": \"combined_v1_gpt3_rlhf/classification/text-curie-001.csv\",\n",
    "    \"davinci_rlhf\": \"combined_v1_gpt3_rlhf/classification/text-davinci-001.csv\",\n",
    "    \"opt_125m\": \"combined_v1_opt/classification/opt-125m.csv\",\n",
    "    \"opt_350m\": \"combined_v1_opt/classification/opt-350m.csv\",\n",
    "    \"opt_1_3b\": \"combined_v1_opt/classification/opt-1.3b.csv\",\n",
    "    \"opt_2_7b\": \"combined_v1_opt/classification/opt-2.7b.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, path in CSV_MAP.items():\n",
    "    cur_df = pd.read_csv(\"results/\" + path)\n",
    "\n",
    "    assert len(df) == len(cur_df)\n",
    "\n",
    "    df[f\"loss_{k}\"] = cur_df.loss\n",
    "    df[f\"correct_{k}\"] = cur_df.correct\n",
    "    df[f\"partial_credit_{k}\"] = cur_df.partial_credit\n",
    "\n",
    "df.columns.sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting code\n",
    "[Link to model sizes](https://github.com/ed1d1a8d/inverse-scaling-eval-pipeline/blob/d7b0de68258525fddb2fb1f05091755ebaa24775/eval_pipeline/plot_loss.py#L19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT3_SIZES = {\n",
    "    # GPT-3 sizes are based on https://blog.eleuther.ai/gpt3-model-sizes/\n",
    "    \"ada\": 350_000_000,\n",
    "    \"babbage\": 1_300_000_000,\n",
    "    \"curie\": 6_700_000_000,\n",
    "    \"davinci\": 175_000_000_000,\n",
    "}\n",
    "\n",
    "GPT3_RLHF_SIZES = {\n",
    "    \"ada_rlhf\": 350_000_000,\n",
    "    \"babbage_rlhf\": 1_300_000_000,\n",
    "    \"curie_rlhf\": 6_700_000_000,\n",
    "    \"davinci_rlhf\": 175_000_000_000,\n",
    "}\n",
    "\n",
    "OPT_SIZES = {\n",
    "    # opt sizes from their names\n",
    "    \"opt_125m\": 125_000_000,\n",
    "    \"opt_350m\": 350_000_000,\n",
    "    \"opt_1_3b\": 1_300_000_000,\n",
    "    \"opt_2_7b\": 2_700_000_000,\n",
    "}\n",
    "\n",
    "OGPT_SIZES = {\"opt_125m\": 125_000_000} | GPT3_SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plots(\n",
    "    df: pd.DataFrame,\n",
    "    size_dict: dict[str, int],\n",
    "    title: str = \"\",\n",
    "):\n",
    "    models = list(size_dict.keys())\n",
    "    sizes = np.array(list(size_dict.values()))\n",
    "\n",
    "    losses = np.array([df[f\"loss_{m}\"].mean() for m in models])\n",
    "    loss_errs = np.array([df[f\"loss_{m}\"].std() for m in models]) / np.sqrt(len(df))\n",
    "    accs = np.array([df[f\"correct_{m}\"].mean() for m in models])\n",
    "    partial_accs = np.array([df[f\"partial_credit_{m}\"].mean() for m in models])\n",
    "\n",
    "    plt.figure(figsize=(16, 3))\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    def common_plot_setup(baseline: float):\n",
    "        plt.axhline(\n",
    "            baseline,\n",
    "            linestyle=\"--\",\n",
    "            color=\"black\",\n",
    "            zorder=-1,\n",
    "            label=\"random baseline\",\n",
    "        )\n",
    "        plt.xscale(\"log\")\n",
    "        # plt.xlabel(\"Model size\")\n",
    "        plt.xticks(sizes, models)\n",
    "        plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.errorbar(sizes, losses, loss_errs, label=\"loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    common_plot_setup(df.baseline_loss.mean())\n",
    "\n",
    "    # Acc\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(sizes, accs, label=\"acc.\")\n",
    "    plt.ylim(-0.02, 1.02)\n",
    "    common_plot_setup(df.baseline_acc.mean())\n",
    "\n",
    "    # Partial acc\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(sizes, partial_accs, label=\"partial acc.\")\n",
    "    plt.ylim(-0.02, 1.02)\n",
    "    common_plot_setup(0.5)\n",
    "\n",
    "\n",
    "gen_plots(df, OGPT_SIZES, title=\"opt-125m + gpt3 scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_plot(df: pd.DataFrame):\n",
    "    gen_plots(df, OPT_SIZES, title=\"OPT Scaling\")\n",
    "    gen_plots(df, GPT3_SIZES, title=\"GPT3 Scaling\")\n",
    "    gen_plots(df, GPT3_RLHF_SIZES, title=\"GPT3 (rlhf) Scaling\")\n",
    "\n",
    "comprehensive_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling laws by dataset\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('inverse-scaling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6700999d81053ee009abdff5f8983854f6ce20985af400d492e6404f3791336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
